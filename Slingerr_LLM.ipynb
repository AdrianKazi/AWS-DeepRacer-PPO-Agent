{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2kew9sBePDuxRI2FVatdv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdrianKazi/AWS-DeepRacer-PPO-Agent/blob/main/Slingerr_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Prod) Data"
      ],
      "metadata": {
        "id": "be-sZrahELr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports data from provided links and store in data/corpus.txt"
      ],
      "metadata": {
        "id": "6eXXBhp3H2Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "def fetch_corpus(output_path=\"data/corpus.txt\", timeout=30):\n",
        "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    DATA_SOURCES = {\n",
        "        # TECH / ENGINEERING LANGUAGE\n",
        "        \"github_opensource_guide\": \"https://raw.githubusercontent.com/github/opensource.guide/main/README.md\",\n",
        "        \"kubernetes_intro\": \"https://raw.githubusercontent.com/kubernetes/website/main/content/en/docs/concepts/overview/what-is-kubernetes.md\",\n",
        "        \"docker_docs\": \"https://raw.githubusercontent.com/docker/docs/main/README.md\",\n",
        "\n",
        "        # PROGRAMMING / Q&A STYLE\n",
        "        \"reddit_programming\": \"https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit-programming.txt\",\n",
        "\n",
        "        # JOB / ROLE / CORP LANGUAGE (synthetic but legit)\n",
        "        \"job_postings\": \"https://raw.githubusercontent.com/IBM/dataset-job-postings/master/data/job_postings.txt\",\n",
        "    }\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for name, url in DATA_SOURCES.items():\n",
        "            print(f\"Fetching {name}\")\n",
        "            try:\n",
        "                r = requests.get(url, timeout=timeout)\n",
        "                r.raise_for_status()\n",
        "                f.write(r.text)\n",
        "                f.write(\"\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"SKIPPED {name}: {e}\")\n",
        "\n",
        "    print(\"DONE. Corpus at:\", output_path)\n",
        "\n",
        "# usage:\n",
        "data = fetch_corpus()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-OUoDlhFmxN",
        "outputId": "589c18d6-ddac-425a-c26e-a6018f9c6e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching github_opensource_guide\n",
            "Fetching kubernetes_intro\n",
            "SKIPPED kubernetes_intro: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/kubernetes/website/main/content/en/docs/concepts/overview/what-is-kubernetes.md\n",
            "Fetching docker_docs\n",
            "Fetching reddit_programming\n",
            "SKIPPED reddit_programming: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit-programming.txt\n",
            "Fetching job_postings\n",
            "SKIPPED job_postings: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/IBM/dataset-job-postings/master/data/job_postings.txt\n",
            "DONE. Corpus at: data/corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    print(f.read(2000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3OH96pWGNS5",
        "outputId": "a5244737-de1f-4c23-f810-85d8afb72bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Open Source Guides\n",
            "[![Build Status](https://github.com/github/opensource.guide/workflows/GitHub%20Actions%20CI/badge.svg)](https://github.com/github/opensource.guide/actions)\n",
            "\n",
            "Open Source Guides (https://opensource.guide/) are a collection of resources for individuals, communities, and companies who want to learn how to run and contribute to an open-source project.\n",
            "\n",
            "## Background\n",
            "Open Source Guides were created and are curated by GitHub, along with input from outside community reviewers, but they are not exclusive to GitHub products. One reason we started this project is that we felt that there weren't enough resources for people creating open-source projects.\n",
            "\n",
            "Our goal was to aggregate community best practices, *not* what GitHub (or any other individual or entity) thinks is best. Therefore, we used examples and quotations from others to illustrate our points.\n",
            "\n",
            "## Contributing\n",
            "\n",
            "This site is powered by [Jekyll](https://jekyllrb.com/). Check out our [contributing guidelines](/CONTRIBUTING.md) for ways to offer feedback and contribute.\n",
            "\n",
            "## Licenses\n",
            "\n",
            "Content is released under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). See [notices](notices.md) for complete details, including attribution guidelines, contribution terms, and software and third-party licenses and permissions.\n",
            "\n",
            "## Acknowledgments\n",
            "\n",
            "The initial release of these guides were authored by **[@nayafia][1], [@bkeepers][2], [@stephbwills][3],** and **[@mlinksva][4]**.\n",
            "\n",
            "Thanks to **[@aitchabee][5], [@benbalter][6], [@brettcannon][7], [@caabernathy][8], [@coralineada][9], [@dmleong][10], [@ericholscher][11], [@gr2m][12], [@janl][13], [@jessfraz][14], [@bluesomewhere][15], [@kfogel][16], [@kytrinyx][17], [@lee-dohm][18], [@mikeal][19], [@mikemcquaid][20], [@nathansobo][21], [@nruff][22], [@nsqe][23], [@orta][24], [@parkr][25], [@shazow][26], [@steveklabnik][27],** and **[@wooorm][28]** for lending their valuable input and expertise leading up to the initial release, and to **[@sophshep][29]** and **[@j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "DZ8AGoUWEMos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Own Tokenizer"
      ],
      "metadata": {
        "id": "TV27_0-KMeSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding algorithm on corpus data to build tokenizer"
      ],
      "metadata": {
        "id": "uefKtoElH9FP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpKcsbyRDzqp",
        "outputId": "96eadddb-6867-40a9-f5f1-18d91d709989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE → slingerra_tok.model / slingerra_tok.vocab\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "\n",
        "def train_tokenizer(\n",
        "    corpus_path=\"data/corpus.txt\",\n",
        "    model_prefix=\"slingerra_tok\",\n",
        "    vocab_size=2000,\n",
        "):\n",
        "    corpus_path = Path(corpus_path)\n",
        "    assert corpus_path.exists() and corpus_path.stat().st_size > 0, \"Corpus missing or empty\"\n",
        "\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=str(corpus_path),\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        model_type=\"bpe\",\n",
        "        byte_fallback=True,\n",
        "        character_coverage=1.0,\n",
        "        normalization_rule_name=\"identity\"\n",
        "    )\n",
        "\n",
        "    print(f\"DONE → {model_prefix}.model / {model_prefix}.vocab\")\n",
        "\n",
        "# usage:\n",
        "train_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Tokenizer"
      ],
      "metadata": {
        "id": "IcK8rIHnIrlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"slingerra_tok.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdBNXUxGIN5y",
        "outputId": "dc2dc816-c323-4341-c768-7392399af4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Tokenizer"
      ],
      "metadata": {
        "id": "pLuEdtQyItDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Senior Python engineer with AWS and Docker experience\"\n",
        "\n",
        "ids = sp.encode(text, out_type=int)\n",
        "tokens = sp.encode(text, out_type=str)\n",
        "\n",
        "print(ids)\n",
        "print(tokens)\n",
        "\n",
        "decoded = sp.decode(ids)\n",
        "print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hLyY_BwIPem",
        "outputId": "82fc98b3-691c-4ec7-a0c2-c7a44953e6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[343, 281, 1649, 414, 1949, 259, 284, 885, 1941, 604, 262, 1044, 809, 1980, 1967, 301, 351, 491, 1937, 1611, 281, 1352]\n",
            "['▁S', 'en', 'ior', '▁P', 'y', 'th', 'on', '▁en', 'g', 'ine', 'er', '▁with', '▁A', 'W', 'S', '▁and', '▁Docker', '▁ex', 'p', 'eri', 'en', 'ce']\n",
            "Senior Python engineer with AWS and Docker experience\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Prod) GPT2"
      ],
      "metadata": {
        "id": "zp9i8Sw4MYXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"Senior Python engineer with AWS and Docker experience welding machine and aerospace engieenering based in Ukraine AWS Lambda\"\n",
        "ids = tokenizer.encode(text)\n",
        "decoded = tokenizer.decode(ids)\n",
        "\n",
        "print(ids)\n",
        "print(tokenizer.tokenize(text))\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozdf7TLWMYKg",
        "outputId": "e9ffb46c-f5cd-4ebf-bbb2-4086bc1e6a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31224, 11361, 11949, 351, 30865, 290, 25716, 1998, 47973, 4572, 290, 40439, 1786, 494, 877, 278, 1912, 287, 7049, 30865, 21114, 6814]\n",
            "['Senior', 'ĠPython', 'Ġengineer', 'Ġwith', 'ĠAWS', 'Ġand', 'ĠDocker', 'Ġexperience', 'Ġwelding', 'Ġmachine', 'Ġand', 'Ġaerospace', 'Ġeng', 'ie', 'ener', 'ing', 'Ġbased', 'Ġin', 'ĠUkraine', 'ĠAWS', 'ĠLamb', 'da']\n",
            "Senior Python engineer with AWS and Docker experience welding machine and aerospace engieenering based in Ukraine AWS Lambda\n"
          ]
        }
      ]
    }
  ]
}